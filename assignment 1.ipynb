{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7217a2",
   "metadata": {},
   "source": [
    "# ANS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c216d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Header\n",
      "0                                  Contents\n",
      "1                              Web scraping\n",
      "2                             History[edit]\n",
      "3                          Techniques[edit]\n",
      "4                Human copy-and-paste[edit]\n",
      "5               Text pattern matching[edit]\n",
      "6                    HTTP programming[edit]\n",
      "7                        HTML parsing[edit]\n",
      "8                         DOM parsing[edit]\n",
      "9                Vertical aggregation[edit]\n",
      "10    Semantic annotation recognizing[edit]\n",
      "11  Computer vision web-page analysis[edit]\n",
      "12                           Software[edit]\n",
      "13                       Legal issues[edit]\n",
      "14                      United States[edit]\n",
      "15                     European Union[edit]\n",
      "16                          Australia[edit]\n",
      "17                              India[edit]\n",
      "18    Methods to prevent web scraping[edit]\n",
      "19                           See also[edit]\n",
      "20                         References[edit]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "headers = []\n",
    "for header_tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    headers.append(header_tag.text)\n",
    "\n",
    "df = pd.DataFrame(headers, columns=['Header'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5e643",
   "metadata": {},
   "source": [
    "# ANS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74aa86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         name  year  rating\n",
      "0    The Shawshank Redemption  1994     9.2\n",
      "1               The Godfather  1972     9.2\n",
      "2             The Dark Knight  2008     9.0\n",
      "3       The Godfather Part II  1974     9.0\n",
      "4                12 Angry Men  1957     9.0\n",
      "..                        ...   ...     ...\n",
      "245               Dersu Uzala  1975     8.0\n",
      "246                  The Help  2011     8.0\n",
      "247                   Aladdin  1992     8.0\n",
      "248                    Gandhi  1982     8.0\n",
      "249        Dances with Wolves  1990     8.0\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "movies = []\n",
    "rows = soup.find_all('tr')[1:]\n",
    "for row in rows:\n",
    "    movie = {}\n",
    "    movie['name'] = row.find('td', {'class': 'titleColumn'}).find('a').text\n",
    "    movie['year'] = int(row.find('td', {'class': 'titleColumn'}).find('span').text[1:5])\n",
    "    movie['rating'] = float(row.find('td', {'class': 'ratingColumn imdbRating'}).find('strong').text)\n",
    "    movies.append(movie)\n",
    "\n",
    "df = pd.DataFrame(movies)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8790a33",
   "metadata": {},
   "source": [
    "# ANS 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f61eab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    name  year  rating\n",
      "0    Ramayana: The Legend of Prince Rama  1993     8.5\n",
      "1             Rocketry: The Nambi Effect  2022     8.4\n",
      "2                                Nayakan  1987     8.4\n",
      "3                               Gol Maal  1979     8.4\n",
      "4                             Anbe Sivam  2003     8.4\n",
      "..                                   ...   ...     ...\n",
      "245                       Poove Unakkaga  1996     7.7\n",
      "246                        Minnal Murali  2021     7.7\n",
      "247                    Thiruchitrambalam  2022     7.7\n",
      "248                           Goodachari  2018     7.7\n",
      "249                                 Joji  2021     7.6\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "movies = []\n",
    "rows = soup.find_all('tr')[1:]\n",
    "for row in rows:\n",
    "    movie = {}\n",
    "    movie['name'] = row.find('td', {'class': 'titleColumn'}).find('a').text\n",
    "    movie['year'] = int(row.find('td', {'class': 'titleColumn'}).find('span').text[1:5])\n",
    "    movie['rating'] = float(row.find('td', {'class': 'ratingColumn imdbRating'}).find('strong').text)\n",
    "    movies.append(movie)\n",
    "\n",
    "df = pd.DataFrame(movies)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee402e35",
   "metadata": {},
   "source": [
    "# ANS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a31785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.html'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "presidents = []\n",
    "rows = soup.find_all('tr')[1:]\n",
    "for row in rows:\n",
    "    president = {}\n",
    "    president['name'] = row.find('td').text.strip()\n",
    "    president['term_of_office'] = row.find_all('td')[1].text.strip()\n",
    "    presidents.append(president)\n",
    "\n",
    "df = pd.DataFrame(presidents)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2c220",
   "metadata": {},
   "source": [
    "# ANS 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fca3d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '5,010'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m team[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     15\u001b[0m team[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m---> 16\u001b[0m team[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m team[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m     18\u001b[0m teams\u001b[38;5;241m.\u001b[39mappend(team)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '5,010'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "teams = []\n",
    "rows = soup.find('table', {'class': 'table'}).find('tbody').find_all('tr')[:10]\n",
    "for row in rows:\n",
    "    team = {}\n",
    "    team['name'] = row.find_all('td')[1].text.strip()\n",
    "    team['matches'] = int(row.find_all('td')[2].text.strip())\n",
    "    team['points'] = int(row.find_all('td')[3].text.strip())\n",
    "    team['rating'] = int(row.find_all('td')[4].text.strip())\n",
    "    teams.append(team)\n",
    "\n",
    "df_teams = pd.DataFrame(teams)\n",
    "\n",
    "batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "\n",
    "response = requests.get(batsmen_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "batsmen = []\n",
    "rows = soup.find('table', {'class': 'table'}).find('tbody').find_all('tr')[:10]\n",
    "for row in rows:\n",
    "    batsman = {}\n",
    "    batsman['name'] = row.find_all('td')[1].text.strip()\n",
    "    batsman['team'] = row.find_all('td')[2].text.strip()\n",
    "    batsman['rating'] = int(row.find_all('td')[3].text.strip())\n",
    "    batsmen.append(batsman)\n",
    "\n",
    "df_batsmen = pd.DataFrame(batsmen)\n",
    "\n",
    "bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "response = requests.get(bowlers_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "bowlers = []\n",
    "rows = soup.find('table', {'class': 'table'}).find('tbody').find_all('tr')[:10]\n",
    "for row in rows:\n",
    "    bowler = {}\n",
    "    bowler['name'] = row.find_all('td')[1].text.strip()\n",
    "    bowler['team'] = row.find_all('td')[2].text.strip()\n",
    "    bowler['rating'] = int(row.find_all('td')[3].text.strip())\n",
    "    bowlers.append(bowler)\n",
    "\n",
    "df_bowlers = pd.DataFrame(bowlers)\n",
    "\n",
    "print(df_teams)\n",
    "print(df_batsmen)\n",
    "print(df_bowlers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98489010",
   "metadata": {},
   "source": [
    "# ANS 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b468cb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpanda\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      5\u001b[0m url\u001b[38;5;241m=\u001b[39m icc\u001b[38;5;241m-\u001b[39mcricket\u001b[38;5;241m.\u001b[39mcom\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'panda'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import panda as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url= icc-cricket.com\n",
    "\n",
    "response = request.get(url) \n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "teams = []\n",
    "rows = soup.find('table', {'class': 'table'}).find('tbody').find_all('tr')[:10]\n",
    "for row in rows:\n",
    "    team = {}\n",
    "    team['name'] = row.find_all('td')[1].text.strip()\n",
    "    team['matches'] = int(row.find_all('td')[2].text.strip())\n",
    "    team['points'] = int(row.find_all('td')[3].text.strip())\n",
    "    team['rating'] = int(row.find_all('td')[4].text.strip())\n",
    "    teams.append(team)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81c5c4",
   "metadata": {},
   "source": [
    "# ANS 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb5c5ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'beautifulsoup' from 'bs4' (C:\\Users\\piyus\\anaconda3\\lib\\site-packages\\bs4\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m beautifulsoup\n\u001b[0;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world/Web_scraping\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mget(url)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'beautifulsoup' from 'bs4' (C:\\Users\\piyus\\anaconda3\\lib\\site-packages\\bs4\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import beautifulsoup\n",
    "url = 'https://www.cnbc.com/world/?region=world/Web_scraping'\n",
    "\n",
    "response = request.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f571ee",
   "metadata": {},
   "source": [
    "# ANS 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d4a8844",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (809010466.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [17]\u001b[1;36m\u001b[0m\n\u001b[1;33m    url = https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import beautifulsoup\n",
    "url = https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "    response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'sc-1qrq3sd-0 sc-orwwe2-4 iUciVs fuwoaH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4526699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
